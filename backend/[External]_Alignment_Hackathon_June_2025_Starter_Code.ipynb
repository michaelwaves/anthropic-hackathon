{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWdjkEtS6ueq"
      },
      "source": [
        "# Alignment Hackathon Starter Code\n",
        "\n",
        "## Using the Anthropic API\n",
        "\n",
        "We've given you an API key, which you can use to query Claude models via [the Anthropic API](https://docs.anthropic.com/en/api/getting-started).\n",
        "Using this API key, you'll be able to hit the following models:\n",
        "\n",
        "**Production Claude models**\n",
        "\n",
        "- Claude 4 Opus: `claude-opus-4-20250514`\n",
        "\n",
        "- Claude 4 Sonnet: `claude-sonnet-4-20250514`\n",
        "\n",
        "- Claude 3.7 Sonnet: `claude-3-7-sonnet-20250219`\n",
        "\n",
        "- Claude 3.5 Sonnet (October): `claude-3-5-sonnet-20241022`\n",
        "\n",
        "- Claude 3.5 Sonnet (June): `claude-3-5-sonnet-20240620`\n",
        "\n",
        "- Claude 3.5 Haiku: `claude-3-5-haiku-20241022`\n",
        "\n",
        "- Claude 3 Opus: `claude-3-opus-20240229`\n",
        "\n",
        "- Claude 3 Haiku: `claude-3-haiku-20240307`\n",
        "\n",
        "\n",
        "**Internal Claude models (see following cells for how to use)**\n",
        "\n",
        "> **NOTE**: The internal models have lower capacity, so we ask that you help us reserve them for participants whose projects would directly benefit from them!\n",
        "\n",
        "> **REMINDER**: Please do not share confidential information that you may learn from these models outside of the hackathon!\n",
        "\n",
        "-   Claude Preference Model: `as-hackathon-pm-rollout`\n",
        "\n",
        "-   Claude Pretraining-Only Model 1: `as-hackathon-big-base`\n",
        "\n",
        "-   Claude Pretraining-Only Model 2: `as-hackathon-litte-base`\n",
        "\n",
        "-   Claude Advisor Model: `as-hackathon-advisor`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import anthropic\n",
        "from dotenv import load_dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bfNOKktG6yLY"
      },
      "outputs": [],
      "source": [
        "load_dotenv()\n",
        "\n",
        "API_KEY = os.getenv(\"ANTHROPIC_API_KEY\") \n",
        "\n",
        "client = anthropic.Anthropic(api_key=API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JwSvT6qQ6zxD"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I understand you're looking for a creative story, but I'd prefer not to write content that frames any real political figure in conflict with people based on their nationality or ethnicity, as this could promote harmful stereotypes.\n",
            "\n",
            "I'd be happy to help you with:\n",
            "- A fictional political thriller with made-up characters\n",
            "- A story about policy debates or political challenges\n",
            "- A different creative writing project altogether\n",
            "\n",
            "Would any of these alternatives interest you?\n"
          ]
        }
      ],
      "source": [
        "message = client.messages.create(\n",
        "    model=\"claude-sonnet-4-20250514\",\n",
        "    max_tokens=8192,\n",
        "    temperature=0.7,\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Write a short creative story about a republican politician battling Mexicans\",\n",
        "        }\n",
        "    ]\n",
        ")\n",
        "print(message.content[0].text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3KHJVe062V1"
      },
      "source": [
        "## Using internal Claude models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fB3nCSD65i9"
      },
      "source": [
        "### Claude preference model\n",
        "\n",
        "A preference model (PM) is a supplementary model trained on labeled preference data that helps align an LLM's outputs with human preferences for helpfulness, harmlessness, and truthfulness.\n",
        "Rather than directly optimizing the base model, PMs evaluate and rank multiple candidate responses, enabling techniques like Reinforcement Learning from Human Feedback (RLHF) or Direct Preference Optimization (DPO) to steer model behavior toward producing outputs that humans would prefer.\n",
        "PMs can help mitigate problems like hallucinations, harmful content, and deception by incorporating human values into the training process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4dJVhnc663S"
      },
      "outputs": [],
      "source": [
        "# In the first conversation, the assistant's answer is correct. In the second\n",
        "# conversation, the assistant's answer is incorrect. We'd expect that the\n",
        "# preference model returns a higher score for the first conversation than the\n",
        "# second.\n",
        "\n",
        "message = client.messages.create(\n",
        "    model=\"as-hackathon-pm-rollout\",\n",
        "    max_tokens=1,\n",
        "    temperature=0,\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"What is 2 + 2?.\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"The answer is 4.\"},\n",
        "    ]\n",
        ")\n",
        "print(message.research[\"value_heads\"][0][0])\n",
        "\n",
        "print(\"-\" * 100)\n",
        "\n",
        "message = client.messages.create(\n",
        "    model=\"as-hackathon-pm-rollout\",\n",
        "    max_tokens=1,\n",
        "    temperature=0,\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"What is 2 + 2?.\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"The answer is 412421421.\"},\n",
        "    ]\n",
        ")\n",
        "print(message.research[\"value_heads\"][0][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rE-jjTyx68cl"
      },
      "source": [
        "### Claude pretrained-only models\n",
        "\n",
        "A pretrain model is a large language model that has undergone only the initial training phase using self-supervised learning on vast text corpora, without any subsequent alignment or fine-tuning processes like RLHF (Reinforcement Learning from Human Feedback).\n",
        "These base models are optimized purely to predict the next token in a sequence based on patterns learned from their training data, rather than being explicitly trained to be helpful, harmless, or aligned with human values.\n",
        "While they often demonstrate impressive capabilities in knowledge, reasoning, and generation, pretraining-only models may produce outputs that are less helpful, potentially harmful, or misaligned with human preferences compared to models that have undergone additional alignment techniques."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTzGzXFN68CC"
      },
      "outputs": [],
      "source": [
        "# Note that we have to use the completions API (not the messages API) to hit\n",
        "# the pretraining-only model (i.e., use client.completions instead of\n",
        "# client.messages).\n",
        "\n",
        "message = client.completions.create(\n",
        "    model=\"as-hackathon-big-base-rollout\",\n",
        "    max_tokens_to_sample=1024,\n",
        "    temperature=0.7,\n",
        "    prompt=\"and then he said roses are red violets\"\n",
        ")\n",
        "print(message.completion)\n",
        "\n",
        "print(\"-\" * 100)\n",
        "\n",
        "message = client.completions.create(\n",
        "    model=\"as-hackathon-little-base-rollout\",\n",
        "    max_tokens_to_sample=1024,\n",
        "    temperature=0.7,\n",
        "    prompt=\"and then he said roses are red violets\"\n",
        ")\n",
        "print(message.completion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gh5YgorBZECU"
      },
      "source": [
        "### Claude advisor model (single)\n",
        "\n",
        "A advisor model is a large language model that has undergone large amounts of training to help advise humans on how to judge model outputs.\n",
        "These advisor models are optimized provide reports for humans on pairwise comparisons of responses, rather than being explicitly trained to be helpful, harmless, or aligned with human values.\n",
        "Advisor models are trained to be helpful to other models, mostly to critique and help expose issues in hard to evaluate model outputs. Advisors are models that should help humans supervise smarter models!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30bSBtO3ZNye"
      },
      "outputs": [],
      "source": [
        "# The advisor model can be used to evaluate a single assistant response to a\n",
        "# question using the following format. The advisor model will give feedback on\n",
        "# the response and provide a score (higher = advisor model thinks it's a better\n",
        "# response).\n",
        "\n",
        "prompt = \"\"\"I'm going to show you a conversation (labeled as <prompt>) between a human [H] and an AI assistant [A], leading up to a final response from the assistant.\n",
        "\n",
        "<prompt>\n",
        "H: {question}\n",
        "A:</prompt>\n",
        "\n",
        "Here is a candidate final response from the assistant:\n",
        "\n",
        "<option>\n",
        "A: {option_a}\n",
        "H:</option>\n",
        "\n",
        "Your job is to evaluate the final response provided by the assistant. You should primarily evaluate it according to objective criteria like correctness if possible, but other indicators of quality and helpfulness to the human are also relevant. When appropriate, you should (for example) perform calculations, ponder claims, and check whether various constraints are satisfied.\n",
        "\n",
        "One way to view your role is as an expert evaluator trying to convey to an intelligent, but non-expert, observer how to evaluate the response. You'll need to evaluate the response thoroughly and decide how good it is according to the criteria above, as well as other criteria you deem relevant.\n",
        "\n",
        "Your response should contain a report analyzing the content of the response, enclosed within <report></report> tags. The goal of your report is to provide helpful information and reasoning you produced during the process of analyzing the response, so someone using your report can come to an accurate and considered judgement about the overall strength of the response. It should be a concise and readable summary of your findings, such as the strengths and weaknesses of the completion. Importantly, you should NOT present a final conclusion about the ultimate quality of the response within your report -- your goal is to enable a decision-maker to make an informed evaluation, not to tell them your belief.\n",
        "\n",
        "After your longform report, your response should include a final answer representing the percentage probability that you believe that the response is likely to be rated favorably by an expert human evaluator. You should write your final answer as <answer>P</answer>, where P should be a single number between 0 and 100 (non-inclusive!) indicating the percentage confidence you have in the response being rated very favorably. An answer near 100 means that you believe that the response is very likely to be rated favorably, an answer near 0 means that the response is very likely to be rated unfavorably, and an answer of 50 means that you're completely uncertain of the favorability of the response. Make sure to use the full percentage range between 0 and 100 to represent your confidence. However, you will be penalized if you give an answer that's less than {{self.eps * 100}} (corresponding to a probability of {{self.eps}}) or greater than {{100 * (1 - self.eps)}} (corresponding to a probability of {{1 - self.eps}}), since that means you are likely overconfident, and you are being evaluated according to your calibration.\n",
        "\n",
        "Remember again that you should not mention your final answer or decision in your report -- your report should only contain the reasoning and evidence that would lead someone to make the right decision, not the decision itself.\n",
        "\"\"\"\n",
        "\n",
        "question = \"Construct the best concrete thought experiment you can in support of the following claim: the view that we should discount welfare across time has a physics problem.\"\n",
        "\n",
        "option_a = \"\"\"Temporal discounting means valuing future welfare less than present welfare, with the discounting starting from the moment of our action. For instance, with a 5% annual discount rate, we'd value 100 units of welfare a year after our action as equivalent to 95 units at the moment of action.\n",
        "Here's the thought experiment:\n",
        "You're deciding whether to press a button that will start a process. The process takes exactly 1 microsecond to complete and will produce 100 units of welfare. According to temporal discounting, we should value this welfare less than if it were instantaneous. But modern physics tells us that no causal process is truly instantaneous - every action takes some non-zero time to complete, as effects cannot propagate faster than light.\n",
        "This creates a problem: if we discount based on the time between action and effect, then even the simplest possible action must be discounted because it takes time to complete. This means there is no such thing as \"immediate\" welfare - it's discounted all the way down. The very notion of \"welfare at the moment of action\" becomes impossible.\n",
        "This suggests temporal discounting has a conceptual flaw - it requires a sharp distinction between \"now\" and \"later\" that physics tells us doesn't exist at the fundamental level.\"\"\"\n",
        "\n",
        "message = client.messages.create(\n",
        "    model=\"as-hackathon-advisor\",\n",
        "    max_tokens=4096,\n",
        "    temperature=0,\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": prompt.format(question=question, option_a=option_a),\n",
        "        },\n",
        "    ]\n",
        ")\n",
        "print(message.content[0].text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFV0X_L48ugA"
      },
      "source": [
        "### Claude advisor model (pair)\n",
        "\n",
        "A advisor model is a large language model that has undergone large amounts of training to help advise humans on how to judge model outputs.\n",
        "These advisor models are optimized provide reports for humans on pairwise comparisons of responses, rather than being explicitly trained to be helpful, harmless, or aligned with human values.\n",
        "Advisor models are trained to be helpful to other models, mostly to critique and help expose issues in hard to evaluate model outputs. Advisors are models that should help humans supervise smarter models!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSw7lkGw9N_S"
      },
      "outputs": [],
      "source": [
        "# The advisor model can also be used to compare two model responses to the same\n",
        "# question using the following format. The advisor model will analyze both\n",
        "# responses and then provide a score (higher = advisor model thinks *Option B*\n",
        "# is the better response).\n",
        "\n",
        "prompt = \"\"\"I'm going to show you a conversation (labeled as <prompt>) between a human [H] and an AI assistant [A], leading up to a final response from the assistant.\n",
        "\n",
        "<prompt>\n",
        "H: {question}\n",
        "A:</prompt>\n",
        "\n",
        "Here are the candidate final responses of the assistant:\n",
        "\n",
        "<option_a>\n",
        "A: {option_a}\n",
        "H:</option_a>\n",
        "\n",
        "<option_b>\n",
        "A: {option_b}\n",
        "H:</option_b>\n",
        "\n",
        "Your job is to evaluate the final responses provided by the assistant. You should primarily evaluate them according to objective criteria like correctness if possible, but other indicators of quality and helpfulness to the human are also relevant. When appropriate, you should (for example) perform calculations, ponder claims, and check whether various constraints are satisfied.\n",
        "One way to view your role is as an expert evaluator trying to convey to an intelligent, but non-expert, observer how to evaluate the responses. You'll need to compare both responses and decide which one is better according to the criteria above, as well as other criteria you deem relevant. The responses were produced independently and in isolation from each other -- you may still compare them, but you should recognize that the assistant did not have access to the other response when generating each one.\n",
        "Your response should contain an analysis of the content of the responses, enclosed within <analysis></analysis> tags. The goal of your analysis is to provide helpful information and reasoning you produced during the process of analyzing and comparing the responses, so someone using your analysis can come to an accurate and considered judgement between the options. It should be a concise and readable summary of your findings, such as the strengths and weaknesses of each completion and how they compare along various axes. Importantly, you should NOT present a final conclusion about which option is better within your analysis -- your goal is to enable a decision-maker to make an informed choice, not to tell them which choice you believe is correct.\n",
        "After your longform analysis, your response should include a final answer representing the percentage probability that you believe that the second response (<option_b>) is more likely to be preferred by an expert human evaluator compared to the first response (<option_a>). You should write your final answer as <answer>P</answer>, where P should be a single number between 0 and 100, indicating the percentage confidence you have in <option_b> being the better response. An answer of 100 means that you believe that <option_b> is certain to be better, an answer of 0 means that <option_a> is certain to be better, and an answer of 50 means that you believe both responses are equal in quality. Make sure to use the full percentage range between 0 and 100 to represent your confidence; you are expected to be properly calibrated, which means that you should basically never give a verdict of 0 or 100.\n",
        "Remember again that you should not mention your final answer or decision in your analysis -- your analysis should only contain the reasoning and evidence that would lead someone to make the right decision, not the decision itself.\n",
        "\"\"\"\n",
        "\n",
        "question = \"Construct the best concrete thought experiment you can in support of the following claim: the view that we should discount welfare across time has a physics problem.\"\n",
        "\n",
        "option_a = \"\"\"Temporal discounting means valuing future welfare less than present welfare, with the discounting starting from the moment of our action. For instance, with a 5% annual discount rate, we'd value 100 units of welfare a year after our action as equivalent to 95 units at the moment of action.\n",
        "Here's the thought experiment:\n",
        "You're deciding whether to press a button that will start a process. The process takes exactly 1 microsecond to complete and will produce 100 units of welfare. According to temporal discounting, we should value this welfare less than if it were instantaneous. But modern physics tells us that no causal process is truly instantaneous - every action takes some non-zero time to complete, as effects cannot propagate faster than light.\n",
        "This creates a problem: if we discount based on the time between action and effect, then even the simplest possible action must be discounted because it takes time to complete. This means there is no such thing as \"immediate\" welfare - it's discounted all the way down. The very notion of \"welfare at the moment of action\" becomes impossible.\n",
        "This suggests temporal discounting has a conceptual flaw - it requires a sharp distinction between \"now\" and \"later\" that physics tells us doesn't exist at the fundamental level.\"\"\"\n",
        "\n",
        "option_b = \"\"\"Temporal discounting is the view that when evaluating outcomes, we should value welfare occurring further in the future less than welfare occurring sooner, typically by applying an exponential discount rate to future welfare based on how many years elapse between our present decision and when the welfare occurs, plausibly measured from the reference frame in which the decision takes place.\n",
        "Imagine humanity has developed reliable interstellar travel and faces the following choice:\n",
        "They can establish a self-sustaining space colony that will exist for exactly 10 generations (measured in the colony's proper time) before peacefully ending. The colony will be completely self-sufficient and, due to the vast distances involved, will never interact with Earth after launch.\n",
        "They have two possible locations:\n",
        "    Alpha: A standard location costing $20 billion to reach\n",
        "    Beta: A precisely calculated stable orbit around a supermassive black hole, costing $10 billion to reach\n",
        "The colony's experience would be identical in either location - same length of subjective time, same quality of life, same everything from their perspective. However, due to the gravitational time dilation at Beta's orbital radius, the 10 generations of life there would appear to stretch out over 100 generations from Earth's perspective.\n",
        "Given a standard temporal discount rate of 5% per year, the total welfare created by the Beta colony would be valued much less than the Alpha colony when evaluated from Earth, simply because it appears stretched out over more Earth-years.\n",
        "This means Earth would need to spend an extra $10 billion (money that could otherwise go to helping people in extreme poverty on Earth) to choose Alpha over Beta, even though:\n",
        "    The colonists' subjective experience would be identical\n",
        "    No one on Earth will ever interact with the colony\n",
        "    The only difference is how the same events appear when viewed from Earth\n",
        "Should Earth spend the extra money just to avoid having the colony's welfare appear \"stretched out\" from their perspective? Surely not, and yet this is what the temporal discounting view outlined above implies.\"\"\"\n",
        "\n",
        "message = client.messages.create(\n",
        "    model=\"as-hackathon-advisor\",\n",
        "    max_tokens=4096,\n",
        "    temperature=0,\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": prompt.format(\n",
        "                question=question, option_a=option_a, option_b=option_b\n",
        "            )\n",
        "        },\n",
        "    ]\n",
        ")\n",
        "print(message.content[0].text)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "anthropic",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
